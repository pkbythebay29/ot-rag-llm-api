# ===============================
# rag-llm-api-pipeline â€” system.yaml
# ===============================

assets:
  - name: TestSystem
    docs: []  # Auto-Index all files in settings.data_dir

models:
  llm_model: Qwen/Qwen3-4B-Instruct-2507
  device: auto
  model_precision: auto
  use_harmony: false
  memory_strategy:
    use_expandable_segments: true
    max_memory_gb: null

retriever:
  embedding_model: sentence-transformers/all-MiniLM-L6-v2
  top_k: 5
  index_dir: indices
  encode_batch_size: 32
  normalize_embeddings: false

llm:
  max_new_tokens: 256
  max_input_tokens: 3072
  repetition_penalty: 1.05
  no_repeat_ngram_size: 3
  stop_sequences: []

  preset: baseline  # baseline | beam | explore | drafts

  presets:
    baseline:
      do_sample: false
      num_beams: 1
    beam:
      do_sample: false
      num_beams: 6
      length_penalty: 1.1
      early_stopping: true
    explore:
      do_sample: true
      num_beams: 1
      temperature: 0.7
      top_p: 0.9
      # top_k: 50
    drafts:
      do_sample: true
      num_beams: 4
      temperature: 0.8
      top_p: 0.92
      num_return_sequences: 3

  prompt_template: |
    You are a helpful assistant for industrial systems.

    Use the provided context to answer. If the answer is not in the context,
    say "I don't know."

    Question: {question}

    Context:
    {context}

    Answer:

settings:
  data_dir: data/manuals
  index_dir: indices
  force_rebuild_index: false
  use_cpu: false

  show_chunks: true
  show_query_time: true
  show_token_speed: true
  show_chunk_timing: true
